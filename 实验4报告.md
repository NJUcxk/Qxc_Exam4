# 钱勋潮 191250115 实验四

## 任务一

### mapreduce代码设计

#### 框架分析

任务需求：统计`industry`的频率并按照频数降序排列

任务一比较类似与作业5的词频统计，首先使用一个job统计`industry`的频数并且输出到临时文件夹，然后再使用一个job从临时文件中读取数据，将key与value转置，通过限定reduce数目为1来排列转置之后的key，最后输出到目标文件夹。

#### 伪代码

~~~pseudocode
//第一个job的map与reduce
class IndustryMapper
method Map(Object key,Text t)
	word = t.split(",")[10]
	Emit(word,1)

class IntSumReducer
method Reduce(Text word,Int val)
	int sum = 0
    sum += val
    Emit(word,sum)

//第二个job的map与reduce
class InverseMapper
method Map(Word t,Int val)
	Emit(val,t)
	
class SimpleReducer
method Reduce(Int val,Word t)
	Emit(t,val)
~~~

#### 细节设计

**`industry`数据的读取**

csv文件按照`,`隔开，于是对读取的一行`String`按照`,`切割得到一个`String`列表，并且读取列表对应位置的值。因为第一行索引也被读取进去了，所以多加一道判断去除industry的索引名。

~~~java
String str = value.toString();
String[] strList = str.split(",");
if(strList[10].equals("industry"))
	return;
word.set(strList[10]);
context.write(word, one);
~~~

### 运行结果

| industry                       | count |
| ------------------------------ | ----- |
| 金融业                         | 48216 |
| 电力、热力生产供应业           | 36048 |
| 公共服务、社会组织             | 30262 |
| 住宿和餐饮业                   | 26954 |
| 文化和体育业                   | 24211 |
| 信息传输、软件和信息技术服务业 | 24078 |
| 建筑业                         | 20788 |
| 房地产业                       | 17990 |
| 交通运输、仓储和邮政业         | 15028 |
| 采矿业                         | 14793 |
| 农、林、牧、渔业               | 14758 |
| 国际组织                       | 9118  |
| 批发和零售业                   | 8892  |
| 制造业                         | 8864  |

运行参数

`hdfs://localhost:9000/exam4/input  hdfs://localhost:9000/exam4/output`

运行结果eclipse打开是乱码，在notepad++上用UTF-8格式打开就正确了

![image-20211210150557513](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211210150557513.png)

![image-20211210150637429](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211210150637429.png)

## Spark安装与配置

环境与版本：

|软件|版本|
|:-:|:-:|
|   Ubuntu   |   20.04.3 LTS   |
|   Java   |  1.8.0_301    |
|   hadoop   |    3.3.1  |
|   spark   |   3.2.0   |
| python | 3.8.10 |

### spark下载与安装



![image-20211211231050596](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211211231050596.png)

在官网下载spark压缩包，winscp发送到虚拟机，解压到文件夹。

**编辑系统变量**

![image-20211211231405222](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211211231405222.png)

**试运行样例代码**

![image-20211211231501282](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211211231501282.png)

在一堆INFO里找到输出

![image-20211211231546128](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211211231546128.png)

**pyspark使用**

![image-20211211231630776](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211211231630776.png)

### python开发环境安装

安装pip3`sudo apt install python3-pip`

![image-20211212115629608](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211212115629608.png)

安装jupyter`pip install jupyter`

![image-20211212115806981](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211212115806981.png)

在`~/.bashrc`文件最后添加Pyspark driver的环境变量

~~~shell
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'
~~~

运行一下刚刚修改的初始化文件

~~~shell
$ source ~/.bashrc
~~~

此时打开pyspark就自动打开jupyter notebook

![image-20211212120035481](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211212120035481.png)

试运行一个程序，配置成功！

![image-20211212120117133](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211212120117133.png)

## 任务二

### 代码设计

- 首先读取数据到RDD，每一行csv是一个元素

~~~python
import math
lines = sc.textFile('train_data.csv')
header = lines.first()#第一行 
lines = lines.filter(lambda row:row != header)#删除第一行
~~~

- 转化操作，对每个元素按照`,`分隔，另元素为列表中的第三个元素也就是`total_loan`。

~~~python
loan_lines = lines.map(lambda x:float(x.split(',')[2]))
~~~

- 定义一个`pair rdd`的生成函数，键为`(2000,3000)`，值为1

~~~python
def get_pair(x):
    floor = int(x/1000)*1000
    ceil = math.ceil(x/1000)*1000
    if floor == ceil:
        ceil += 1000
    key = '('+str(floor)+','+str(ceil)+')'
    return (key,1)
~~~

- 转化操作，对RDD每个元素应用上述函数，生成`pair rdd`

~~~python
pair_loan_lines = loan_lines.map(get_pair)
~~~

- 行动操作，对`pair rdd`进行按键聚合并输出

~~~python
result = pair_loan_lines.reduceByKey(lambda x, y: x + y)
result.sortByKey().collect()
~~~

### 运行结果

|范围|样本数|
|:-:|:-:|
|(0,1000)| 2|
|(1000,2000)| 4043|
|(2000,3000)|6341|
|(3000,4000)|9317|
|(4000,5000)|10071|
|(5000,6000)|16514|
|(6000,7000)| 15961|
|(7000,8000)| 12789|
|(8000,9000)| 16384|
|(9000,10000)|10458|
|(10000,11000)|27170|
|(11000,12000)| 7472|
|(12000,13000)| 20513|
|(13000,14000)| 5928|
|(14000,15000)| 8888|
|(15000,16000)| 18612|
|(16000,17000)| 11277|
|(17000,18000)| 4388|
|(18000,19000)| 9342|
|(19000,20000)| 4077|
|(20000,21000)|17612|
|(21000,22000)| 5507|
|(22000,23000)| 3544|
|(23000,24000)| 2308|
|(24000,25000)| 8660|
|(25000,26000)| 8813|
|(26000,27000)| 1604|
|(27000,28000)| 1645|
|(28000,29000)| 5203|
|(29000,30000)| 1144|
|(30000,31000)|6864|
|(31000,32000)| 752|
|(32000,33000)| 1887|
|(33000,34000)| 865|
|(34000,35000)| 587|
|(35000,36000)| 11427|
|(36000,37000)| 364|
|(37000,38000)| 59|
|(38000,39000)| 85|
|(39000,40000)| 30|
|(40000,41000)|1493|

![image-20211214155648979](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211214155648979.png)

## 任务三

### 问题1

#### 代码设计

首先读取csv文件到dataframe，然后生成表`loan_table`

~~~python
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .enableHiveSupport().getOrCreate()
df = spark.read.options(header='True', inferSchema='True', delimiter=',') \
  .csv("train_data.csv")
df.createOrReplaceTempView("loan_table")
~~~

用sql语句进行查询

~~~python
result = spark.sql("SELECT employer_type,COUNT(*)/(SELECT count(*) FROM loan_table) AS percentage FROM loan_table GROUP BY (employer_type);")
~~~

#### 运行结果

<img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211215181339375.png" alt="image-20211215181339375" style="zoom:67%;" />

<img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211215181403754.png" alt="image-20211215181403754" style="zoom:67%;" />

### 问题2

#### 代码设计

使用sql语句进行计算

~~~python
result2 = spark.sql("select user_id,year_of_loan*monthly_payment*12-total_loan as total_money from loan_table")
~~~

#### 运行结果

<img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211215182136337.png" alt="image-20211215182136337" style="zoom:67%;" />

<img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211215182203195.png" alt="image-20211215182203195" style="zoom:67%;" />

### 问题3

#### 代码设计

发现用来筛选的条件的数据为字符串，查看所有数据种类，其中代表市场大于五年的有如下几种`'10+ years','6 years','7 years','8 years','9 years'`，所以只需要判断工作年限是否为这几个值中的一个即可。

~~~python
result3 = spark.sql("select user_id,censor_status,work_year from loan_table where work_year in ('10+ years','6 years','7 years','8 years','9 years')")
~~~

#### 运行结果

<img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211215182345912.png" alt="image-20211215182345912" style="zoom: 67%;" />

<img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211215182429879.png" alt="image-20211215182429879" style="zoom:67%;" />

 ## 任务四

### 数据预处理

#### 数据预览与缺失值处理

##### 数据分布情况

`total_loan`数据分布

![image-20211216114729644](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216114729644.png)

`year_of_loan`贷款年限为3年或5年，主要为3年

![image-20211216114921298](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216114921298.png)

`interest`⽹络贷款利率，数据分布，利率真高啊

![image-20211216115123296](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216115123296.png)

`monthly_payment`分期付款⾦额

![image-20211216115219183](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216115219183.png)

`class`网络贷款等级

![image-20211216140442191](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216140442191.png)

`house_exist`是否有房

![image-20211216141300617](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216141300617.png)

`house_loan_status`数据分布

![image-20211216141325232](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216141325232.png)

`debt_loan_ratio`数据分布

![image-20211216144814206](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216144814206.png)

![image-20211216144606028](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216144606028.png)

`pub_dero_bankrup`数据分布

![image-20211216145827321](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216145827321.png)

`recircle_u`数据分布

![image-20211216150002102](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216150002102.png)

`f0`

![image-20211216150042261](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216150042261.png)

`f1`

![image-20211216150120363](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216150120363.png)

`f2`

![image-20211216150140726](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216150140726.png)

`f3`

![image-20211216150210392](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216150210392.png)

`f4`

![image-20211216150235372](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216150235372.png)

`f5`

<img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211216150341095.png" alt="image-20211216150341095" style="zoom:50%;" />

##### 缺失值处理

其中`work_year` `post_code` `debt_loan_ratio` `pub_dero_bankrup` `recircle_u` `f0` `f1` `f2` `f3` `f4` `f5`这些指标存在缺失值，根据统计结果，缺失值处理方式如下：

`work_year`空值17428个，使用0填充

`post_code`空值1个，用众数填充

`debt_loan_ratio`空值84个，使用中位数填充

`pub_dero_bankrup`空值170个，使用中位数填充

`recircle_u`空值196个，使用中位数填充空值

`f系列` `f1`空值26113个，其他`f`值空值15154个，均用中位数填充

#### 数据特征化

1. 特征向量中去除`loan_id`与`user_id`
2. `class`网络贷款等级A-G转换为1-7
3. `sub_class`转换为1-5
4. `work_type` `employment_type` `industry`直接使用*LabelEncoder*编码处理
5. `work_year`小于1年转为0，10+转为10，并转为整型

#### 添加新变量

将任务三问题3计算出来的`total_money`作为新变量加入

### 逻辑回归LR

#### 优缺点分析

优点：

- （模型）模型清晰，背后的概率推导经得住推敲。
- （输出）输出值自然地落在0到1之间，并且有概率意义。
- （参数）参数代表每个特征对输出的影响，可解释性强。
- （简单高效）实施简单，非常高效（计算量小、存储占用低），可以在大数据场景中使用。
- （过拟合）解决过拟合的方法很多，如L1、L2正则化。
- （多重共线性）L2正则化就可以解决多重共线性问题。

缺点：

- （特征相关情况）因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。
- （特征空间）特征空间很大时，性能不好。
- （精度）容易欠拟合，精度不高。

在本例子中，特征向量较大，性能可能受到影响。可能存在多重共线性，需要正则化调优。

#### 原始版本

~~~python
###第一步，对原始数据集进行向量化，得到一个features向量
df_assembler = VectorAssembler(inputCols=[ 'total_loan','year_of_loan','interest','monthly_payment','class', 'sub_class', 'work_type', 'employer_type', 'industry', 'work_year',
 'house_exist', 'house_loan_status', 'censor_status','marriage', 'offsprings', 'use', 'post_code', 'region', 'debt_loan_ratio', 'del_in_18month', 'scoring_low', 'scoring_high', 'pub_dero_bankrup',
 'early_return', 'early_return_amount', 'early_return_amount_3mon',
 'recircle_b', 'recircle_u', 'initial_list_status', 'title','policy_code',
 'f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'total_money'],outputCol='features')
labeled_df = df_assembler.transform(df)


###第二步，划分数据集
labeled_df = labeled_df["is_default","features"]
data_set = labeled_df.select(['features', 'is_default'])
train_df, test_df = data_set.randomSplit([0.8, 0.2])


###第三步，模型训练与预测
log_reg = LogisticRegression(labelCol = 'is_default').fit(train_df)
test_result = log_reg.evaluate(test_df).predictions
evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol='is_default')
print('AUC：', evaluator.evaluate(test_result))
~~~



![image-20211219110806472](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211219110806472.png)

在不进行调参优化的情况下，AUC为0.8。

#### 调优

首先尝试进行L1,L2正则化调优，让模型更具有泛化能力。

但是发现并不能对模型有优化左右，参数越大，效果越差。

------

然后尝试了对`threshold`参数进行修改，也就是逻辑回归模型里，根据输出值划分0，1预测值的阈值。默认为0.5，考虑到在本样本中，1的占比很低，需要，可能在划分标准上进行修改可以更加贴近拟合结果。

~~~python
for i in range(0,40):
    threshold = 0.3+0.01*i
    log_reg_2 = LogisticRegression(threshold = threshold,labelCol = 'is_default').fit(train_df)
    test_result_2 = log_reg_2.evaluate(test_df).predictions
    evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol='is_default')
    print('AUC：'+str(threshold), evaluator.evaluate(test_result_2))
~~~



![image-20211219111502661](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211219111502661.png)

总体来说，效果非常有限，提升在万分之一以内。在这个特征向量构造的情况下，AUC基本上能达到0.801

### 神经网络——多层感知器MLP

#### 优缺点分析

多层感知器（Multilayer Perceptron,缩写MLP）是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。一种被称为反向传播算法方法常被用来训练MLP。多层感知器遵循人类神经系统原理，学习并进行数据预测。它首先学习，然后使用权重存储数据，并使用算法来调整权重并减少训练过程中的偏差，即实际值和预测值之间的误差。主要优势在于其快速解决复杂问题的能力。多层感知的基本结构由三层组成：第一输入层，中间隐藏层和最后输出层，输入元素和权重的乘积被馈给具有神经元偏差的求和结点,主要优势在于其快速解决复杂问题的能力。  MLP是感知器的推广，克服了感知器不能对线性不可分数据进行识别的弱点。

#### 原始版本

输入的变量有38个，输出的变量有2个，中间设置两层隐藏层，第一层节点数60，第二层节点数30。

~~~python
###模型训练
layers = [38,60,30,2]
MLPC_trainer = MultilayerPerceptronClassifier(labelCol='is_default', featuresCol="features", maxIter=100, layers=layers, blockSize=128, seed=1234)
MLPC_model = MLPC_trainer.fit(train_df)
###模型预测
MLPC_predictions = MLPC_model.transform(test_df)
evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol='is_default')
print('AUC：', evaluator.evaluate(MLPC_predictions))
~~~

但是节点数过多，导致内存不够:sob::sob::sob:，于是设置第一层的隐藏层为40，AUC为0.65左右。

因为虚拟机性能限制，无法充分发挥这一算法。

### 随机森林RF

首先尝试了单个决策树

![image-20211218172412900](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211218172412900.png)

但是单个决策树的效果非常不佳，也不想调参了，尝试使用随机森林。

-----

但是在随机森林调参之后，得到启发，可以对决策树调参，设置最深层数为15

![image-20211219141126297](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211219141126297.png)

AUC能够达到0.756，还行吧。

#### 优缺点分析

随机森林是由很多决策树构成的，不同决策树之间没有关联。

当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。

![image-20211219125639462](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211219125639462.png)

1. 一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
2. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

优点：

- 对于很多种资料，它可以产生高准确度的分类器。
- 它可以处理大量的输入变量。
- 它可以在决定类别时，评估变量的重要性。
- 在建造森林时，它可以在内部对于一般化后的误差产生不偏差的估计。
- 它包含一个好方法可以估计丢失的资料，并且，如果有很大一部分的资料丢失，仍可以维持准确度。
- 它提供一个实验方法，可以去侦测variable interactions。
- 对于不平衡的分类资料集来说，它可以平衡误差。
- 它计算各例中的亲近度，对于数据挖掘、侦测离群点和将资料可视化非常有用。
- 使用上述。它可被延伸应用在未标记的资料上，这类资料通常是使用非监督式聚类。也可侦测偏离者和观看资料。
- 学习过程是很快速的。

缺点：

- 据观测，如果一些分类/回归问题的训练数据中存在噪音，随机森林中的数据集会出现过拟合的现象。
- 比决策树算法更复杂，计算成本更高。
- 由于其本身的复杂性，它们比其他类似的算法需要更多的时间来训练。

各个平台之间的对比，spark怎么这么拉:sob::sob::sob::sob::sob::sob::sob::sob::sob:

而且占用内存巨大，导致之后树个数和深度增加就会OutOfMemory

![image-20211219125815402](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211219125815402.png)

#### 原始版本

~~~python
rf_classifier=RandomForestClassifier(labelCol='is_default', featuresCol="features",numTrees=numTrees,maxDepth=maxDepth,impurity=impurity,maxBins=maxBins).fit(train_df)  
rf_predictions=rf_classifier.transform(test_df)
evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol='is_default')
print('AUC：'+str(numTrees)+str(maxDepth)+impurity+str(maxBins), evaluator.evaluate(rf_predictions))
~~~



![image-20211218172526217](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211218172526217.png)

未调参的随机森林，训练结果比逻辑回归好了一些。

#### 超参数搜索优化

本文选择在随机森林算法中比较重要的几个超参数进行调优，分别是：决策树个数`numTrees`，决策树最大深度`maxDepth`，节点的不纯净度测量`impurity`，构建节点时数据分箱数`maxBins`四种。

| 超参数     | 范围                    |
| ---------- | ----------------------- |
| `numTrees` | [20,50,100,150,200,300] |
| `maxDepth` | [5,10,15,20,30,40,50]   |
| `impurity` | ['gini', 'entropy']     |
| `maxBins`  | [24,32,40]              |

~~~python
numTrees_list = [20,50,100,150,200,300]
maxDepth_list = [5,10,15,20,30,40,50]
impurity_list = ['gini', 'entropy']
maxBins_list = [24,32,40]
for numTrees in numTrees_list:
    for maxDepth in maxDepth_list:
        for impurity in impurity_list:
            for maxBins in maxBins_list:
                rf_classifier=RandomForestClassifier(labelCol='is_default', featuresCol="features",numTrees=numTrees,maxDepth=maxDepth,impurity=impurity,maxBins=maxBins).fit(train_df)  
                rf_predictions=rf_classifier.transform(test_df)
                evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol='is_default')
                print('AUC：'+str(numTrees)+str(maxDepth)+impurity+str(maxBins), evaluator.evaluate(rf_predictions))
~~~

![image-20211219131035761](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211219131035761.png)

经过调优，**影响精度的主要的因素**在于`numTrees`和`maxDepth`，在`numTrees=20`和`maxDepth=15` `impurity=entropy` `maxBins=32`时达到0.863

其中对**精度影响最大**的是`maxDepth`，在默认设置为5的情况下AUC为0.82左右，10的时候在0.85左右，15的时候达到最大值，在15附近进行尝试，发现15基本已经达到最佳。

对**精度影响第二大**的是`numTrees`，参与投票的树越多，也越容易得到正确答案。

![image-20211219104939980](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211219104939980.png)

由图可以看出当基分类器的数目超过一定值时，模型的错误率基本收敛，再增加的基分类器的数目，效果基本不会提升，只会是使的代码变慢。

在树的个数达到20时，已经达到较好效果，增加到30，40时基本在上下震荡，也许已经收敛。

在`impurity`的选择上，发现一个有趣的现象，在树的深度较浅时‘gini’的表现优于‘entropy’，在树的深度达到10以上时，‘entropy’的表现优于‘gini’，总体来说差距不大。

在`maxBins`的选择上，同样出现类似现象，当树的深度较浅时，24和40的分箱数优于32，但是在树深度较深的时候，32略微优于其他分箱个数。

### 总结

总共尝试了三类算法。

第一类：**逻辑回归**

逻辑回归的速度最快，也一如既往的稳定，虚拟机的内存也能够胜任所有的参数组合。优化一尝试正则化提升泛化能力，但是并没有带来提升。优化二尝试修改分类的阈值，提升非常有限，仅有万分之一不到。

最终AUC能达到0.801左右

速度：    :star::star::star::star::star:

内存：    :star::star::star::star::star:

准确度：:star::star::star:

第二类：**多层感知器**

多层感知器对性能要求较高，虚拟机难以胜任基本的要求:sob:，勉强跑出来的结果仅有0.65左右。

速度：    :star:

内存：    :star:

准确度：:clown_face:

第三类：**随机森林**

首先尝试了决策树，不调参的效果很差，修改节点最大深度为15时，AUC达到0.756左右。

速度：    :star::star::star:

内存：    :star::star::star:

准确度：:star::star:

随机森林的速度相对较快，并且占用内存也在能接受的程度之内。在优化了决策树个数`numTrees`，决策树最大深度`maxDepth`，节点的不纯净度测量`impurity`，构建节点时数据分箱数`maxBins`四种超参数之后，最终得到了虚拟机能力力所能及的最佳超参数组合。

AUC达到0.863左右。因为虚拟机性能限制，最大深度只能探索到18层，树的数目在深度15层时只能探索到40颗树。假如性能能够优化，AUC提到0.87有可能。

速度：    :star::star:

内存：    :star::star:

准确度：:star::star::star::star::star:

